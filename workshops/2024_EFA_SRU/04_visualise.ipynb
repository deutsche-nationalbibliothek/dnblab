{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802867ca-b815-4a7e-81fd-7ddfb23357b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663c34c-baa4-40f4-8b03-313c0b7669f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"pdf_texts.h5\", key=\"df_texts\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e529d8-a87f-4659-b7de-13ecf194e6cf",
   "metadata": {},
   "source": [
    "## 1. Häufigkeitsanalysen \n",
    "\n",
    "### 1.1. Worthäufigkeiten übergreifend (alle PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09587cff-b23a-4a5b-bf48-50e2a4575a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der zu entfernenden Stopwörter\n",
    "nltk.download('stopwords')\n",
    "stopger = stopwords.words('german')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Hinzufügen weiterer Stopwörter\n",
    "newStopwords = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'vgl', '\\x97', '•', '■', 'v',\n",
    "                'beim', 'de','—','ge','la','be','en','que','el','ten','ver','gen','sei','nen','del','nen', 'se','schen','un','land','te','ei','aires',\n",
    "                'las', 'los', '«']\n",
    "\n",
    "#Erweitern der heruntergeladenen Stopwörter mit den selbst definierten \"newStopwords\":\n",
    "stopger.extend(newStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3a181-82ea-4278-8d43-19d1d893cd0e",
   "metadata": {},
   "source": [
    "##### Schrittweise Herleitung der einzelnen Arbeitsschritte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b0bab-1f01-4560-bfe0-bf5031f91ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alle Inhalte aus der Spalte \"text\" des Dataframes in eine Liste überführen\n",
    "content = df['text'].tolist()\n",
    "\n",
    "# Optional: Konvertierung aller Wörter in Kleinschreibung: \n",
    "content = [text.lower() for text in content]  \n",
    "\n",
    "print(content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ceb4b-7c5c-42d4-a175-1e12ec310103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen von Satzzeichen und Tokenisierung\n",
    "listofthings = []\n",
    "for entry in content:\n",
    "    for c in string.punctuation:\n",
    "        entry = entry.replace(c, \" \")\n",
    "    words = entry.split()\n",
    "    listofthings.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48566a9d-a100-4783-8af6-edc58d41cfe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(listofthings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec26c2a-9000-48c1-818f-d2a3e036f2f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entfernen von Zahlen (bspw. Seitenzahlen, andere Zahlen)\n",
    "words = [word for word in listofthings if not word.isdecimal()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e735d4-41b3-4a1a-b573-239c6749f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stopger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0690c-d064-4876-9577-05824a72091d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entfernen der Stopwörter aus der Gesamtwortliste: \n",
    "tokens_without_sw = [word for word in words if word not in stopger]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360a01f-74c3-46d4-8174-6f516eb9144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zählen der Häufigkeit der verbleibenden Wörter\n",
    "counts = Counter(tokens_without_sw)\n",
    "\n",
    "# Top 20 häufigste Wörter ausgeben\n",
    "top_20_words = counts.most_common(20)\n",
    "top_150_words = counts.most_common(150)\n",
    "print(top_20_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd1971-5b3a-4777-bc33-804c660ec629",
   "metadata": {},
   "source": [
    "Zusammenfassung aller Schritte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9731d1-f9f7-4541-84eb-504de9325980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Inhalte aus der Spalte \"text\" des Dataframes in eine Liste überführen\n",
    "content = df['text'].tolist()\n",
    "# Optional: Konvertierung aller Wörter in Kleinschreibung: \n",
    "content = [text.lower() for text in content]  \n",
    "\n",
    "# Manuelle Tokenisierung und Entfernung von Satzzeichen: \n",
    "listofthings = []\n",
    "for entry in content:\n",
    "    for c in string.punctuation:\n",
    "        entry = entry.replace(c, \" \")\n",
    "    words = entry.split()\n",
    "    listofthings.extend(words)\n",
    "\n",
    "# Entfernen von Zahlen (bspw. Seitenzahlen, andere Zahlen)\n",
    "words = [word for word in listofthings if not word.isdecimal()]\n",
    "\n",
    "# Entfernen der Stopwörter aus der Gesamtwortliste: \n",
    "tokens_without_sw = [word for word in words if word not in stopger]\n",
    "\n",
    "# Zählen der Häufigkeit der verbleibenden Wörter\n",
    "counts = Counter(tokens_without_sw)\n",
    "\n",
    "# Top 20 häufigste Wörter ausgeben\n",
    "top_20_words = counts.most_common(20)\n",
    "top_150_words = counts.most_common(150)\n",
    "print(top_20_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e999ea4-432d-4f0c-85f4-ecc4801a1ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429989a4-3119-4f73-8e62-cff4a0ec8575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d36edb-0a5d-463f-ac2b-de8dc4df10d3",
   "metadata": {},
   "source": [
    "#### Funktion\n",
    "\n",
    "Clean Text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b6926-54b8-4835-b304-6e59f3a11af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(df, stopger):\n",
    "  # Funktion zum Bereinigen eines einzelnen Textes\n",
    "    def process_text(text):\n",
    "        # Konvertiere Text in Kleinbuchstaben\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Entferne Satzzeichen\n",
    "        for c in string.punctuation:\n",
    "            text = text.replace(c, \" \")\n",
    "        \n",
    "        # Tokenisiere den Text\n",
    "        words = text.split()\n",
    "        \n",
    "        # Entferne Zahlen\n",
    "        words = [word for word in words if not word.isdecimal()]\n",
    "        \n",
    "        # Entferne Stopwörter\n",
    "        cleaned_tokens = [word for word in words if word not in stopger]\n",
    "        return cleaned_tokens\n",
    "    \n",
    "    # Wende die Funktion auf jede Zeile der 'text'-Spalte an und speichere das Ergebnis in 'cleaned_tokens'\n",
    "    df['cleaned_tokens'] = df['text'].apply(process_text)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606468c-6b48-4670-a24b-c05f692f80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_and_tokenize(df, stopger)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfbad0-37d5-4ca9-b9d0-2900515995ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(df): \n",
    "\n",
    "    # Alle Inhalte aus der Spalte \"text\" des Dataframes in eine Liste überführen\n",
    "    words = df['cleaned_tokens'].tolist()\n",
    "    content = []\n",
    "    for wordlist in words:\n",
    "        for word in wordlist: \n",
    "            content.append(word)\n",
    "\n",
    "    # Zählen der Häufigkeit der verbleibenden Wörter\n",
    "    counts = Counter(content)\n",
    "    \n",
    "    # Top 20 häufigste Wörter ausgeben\n",
    "    top_20_words = counts.most_common(20)\n",
    "    top_150_words = counts.most_common(150)\n",
    "\n",
    "    #frequencies = {\"top 20\": top_20_words, \"top_150\": top_150_words}\n",
    "    #return frequencies\n",
    "    return top_20_words, top_150_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbfb2f-2fd1-4095-925d-9c22e615861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_frequency(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37718d3-3332-4564-9b46-7288d8269dd7",
   "metadata": {},
   "source": [
    "Da die Funktion 2 Variablen zurückgibt, werden diese hintereinander als Variable in die Liste \"result\" gespeichert, d.h. als erstes Element der Liste wird die Variable `top_20_words` und als zweites Listenelement die Variable `top_150_words` abgelegt. Entsprechend können über die Listenindizes die Variablen angesprochen werden: \n",
    "\n",
    "- result[0] enthält also die Variable `top_20_words`\n",
    "- result[1] enthält die Variable `top_150_words`\n",
    "\n",
    "Analog können weitere Häufigkeiten in der Funktion hinzugefügt und angesprochen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5c1f9-77fa-469f-8b91-8434f08abec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e3201-25a4-422f-996f-c1918d552b0d",
   "metadata": {},
   "source": [
    "### 1.2. Visualisierungen\n",
    "\n",
    "#### 1.2.1. Kreisdiagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a0d3-24cb-40cc-ab54-564d0acad976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere das Kreisdiagramm mit Plotly Express\n",
    "def create_plotly_circle(word_count):\n",
    "    labels = [word[0] for word in word_count]\n",
    "    sizes = [word[1] for word in word_count]\n",
    "        \n",
    "    fig = px.pie(word_count, names=labels, values=sizes, title='Top Wörter', height=500)\n",
    "    fig.show()\n",
    "\n",
    "# Kreisdiagramm erstellen\n",
    "create_plotly_circle(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b23fb-5298-4677-8c9d-445289a0b1de",
   "metadata": {},
   "source": [
    "#### 1.2.2. Balkendiagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca02f7a-6975-413b-b492-a0a76dfb27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen eines Balkendiagramms\n",
    "def create_bar_chart(word_count):\n",
    "    labels = [word[0] for word in word_count]\n",
    "    sizes = [word[1] for word in word_count]\n",
    "    \n",
    "    fig = px.bar(x = labels, y = sizes, height=500)\n",
    "    return fig\n",
    "\n",
    "# Balkendiagramm erstellen\n",
    "fig2 = create_bar_chart(result[0])\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f5cf8-ce35-43de-99d7-ee18432f224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramm als Bild speichern:\n",
    "# fig2.savefig('plot.jpg', format='jpg', dpi=300)  # Als JPG speichern\n",
    "# fig2.savefig('plot.png', format='png', dpi=300)  # Als PNG speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595549f6-06a1-42d7-a4ab-4778523306dc",
   "metadata": {},
   "source": [
    "#### 1.2.3. Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99901048-9d37-44c6-a7a2-cbb8def32004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer Wörterbuch-Datenstruktur für die WordCloud\n",
    "word_freq_dict = dict(result[1])\n",
    "\n",
    "# Erstellen und Anzeigen der WordCloud\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')  # Achsen ausblenden\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15792239-8af5-4a20-979c-8c2d2bba34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern: \n",
    "# wc.to_file('wordcloud.png')  # Speichern als PNG-Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b26ad7-1536-4d7f-beed-dc7cab45971b",
   "metadata": {},
   "source": [
    "## 2.1. N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e03f2-22c2-4c27-8969-74b02b7fcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gramm-Erstellung (z.B. für 2-Gramme)\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c7381-e8c9-4d0f-9e0e-8196c4dde2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne die N-Gramme für alle Texte\n",
    "n = 2  # Für bigrams (2-Gramme), ändere dies für trigramme (n=3) etc.\n",
    "df['ngrams'] = df['cleaned_tokens'].apply(lambda x: generate_ngrams(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633fb92-11f2-41b0-8555-b4d6a86aeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle N-Gramme aus allen Texten kombinieren\n",
    "all_ngrams = [ngram for sublist in df['ngrams'] for ngram in sublist]\n",
    "\n",
    "# Zähle die Häufigkeit der N-Gramme\n",
    "ngram_counts = Counter(all_ngrams)\n",
    "\n",
    "# Wähle die häufigsten 10 N-Gramme aus\n",
    "top_ngrams = ngram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fdc89-8e5a-4d80-b559-2858874341cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine DataFrame für die Top-N-Gramme zur besseren Visualisierung\n",
    "ngram_df = pd.DataFrame(top_ngrams, columns=['ngram', 'count'])\n",
    "ngram_df['ngram'] = ngram_df['ngram'].apply(lambda x: ' '.join(x))  # Umwandeln von Tupeln in Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc70767-d7f8-44c3-a343-c7c0e9960ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der häufigsten N-Gramme mit Plotly Express\n",
    "ngram_df = ngram_df.sort_values(by='count', ascending=True)\n",
    "fig = px.bar(ngram_df, x='count', y='ngram', orientation='h', title=f'Top {n}-Gramme',  labels={'count': 'Häufigkeit', 'ngram': f'{n}-Gramme'},  \n",
    "             color='count',  color_continuous_scale='Viridis',  height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69308c8b-271c-4fbb-884d-9a81915e7213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
