{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f01dd51",
   "metadata": {},
   "source": [
    "# EFA25@DNB\n",
    "## LLM via Schnittstelle ansprechen und rudimentäres RAG-System aufbauen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24aa2fd",
   "metadata": {},
   "source": [
    "Die einzige Bibliothek, die benötigt wird, ist `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217b7ca",
   "metadata": {},
   "source": [
    "Auf der Seite [huggingface.co](https://huggingface.co/) muss ein Konto erstellt werden, um einen Access-Token generieren zu lassen. Dieser Token muss als nächstes eingelesen werden. Es ist zu empfehlen, ihn nicht direkt im Code zu hinterlegen, sondern aus einer extern liegenden Datei einzulesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53052769",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"hf_xxxxxxx\" # Hier Token einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b9031",
   "metadata": {},
   "source": [
    "Über `API-URL` ist die Schnittstelle angegeben. Über `MODEL` wird das Sprachmodell bestimmt, in diesem Fall Qwen2.5 mit 72 Milliarden Parametern. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa34368",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://router.huggingface.co/v1/chat/completions\"\n",
    "MODEL = \"Qwen/Qwen2.5-72B-Instruct:together\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c305b4",
   "metadata": {},
   "source": [
    "### Achtung!\n",
    "*Das Angebot der Sprachmodelle variiert, insbesondere bei denen, die kostenlos verfügbar sind. Sollte das hier verwendete Qwen-Modell in ein paar Monaten nicht mehr angeboten werden, muss bei Huggingface ein anderes Modell gewählt werden. Möglicherweise sind dann Änderungen am Code notwendig.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612bef4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Als nächstes wird die Variable `content` befüllt. Hier befinden sich die Daten, auf die das Sprachmodell zugreifen soll. In diesem Fall sind es (fiktive) Öffnungszeiten der Deutschen Nationalbibliothek ([Link zu den echten Öffnungszeiten](https://www.dnb.de/oeffnungszeiten))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"Antworte auf Deutsch.\n",
    "             Deutsche Nationalbibliothek\n",
    "             Adresse:\n",
    "                 Deutscher Platz 1\n",
    "                 04103 Leipzig            \n",
    "             Öffnungszeiten: \n",
    "                 Montag bis Freitag 09:00 Uhr bis 22:00 Uhr\n",
    "                 Samstag 10:00 Uhr bis 18:00 Uhr\n",
    "                \n",
    "             Deutsche Nationalbibliothek \n",
    "             Adresse:\n",
    "                 Adickesallee 1\n",
    "                 60322 Frankfurt am Main            \n",
    "             Öffnungszeiten: \n",
    "                 Montag bis Freitag 09:00 Uhr bis 16:00 Uhr\n",
    "                 Samstag 09:00 Uhr bis 13:00 Uhr\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758e33b",
   "metadata": {},
   "source": [
    "Alternativ kann auch eine Textdatei eingelesen werden. In `datensets.txt` befinden sich die Namen aller Datensets des DNBLabs (Stand: November 2025) mit ihren Beschreibungen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datensets.txt\", \"r\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6243938",
   "metadata": {},
   "source": [
    "Die Funktion `llm(query)` ist das Herzstück dieses Codes. Hier wird die Verbindung zur Schnittstelle des gewählten Modells hergestellt. \n",
    "\n",
    "Wichtig ist `messages`, wo unter `\"role\": \"system\"` der Systemprompt zu finden ist, der in unserem Fall aus dem besteht, was oben unter `content` zu finden ist. Unter `\"role\": \"user\"` wird der Prompt des Nutzers übergeben. Alle Daten werden bei der Generierung des Outputs durch das Sprachmodell berücksichtigt.\n",
    "\n",
    "Darunter finden sich die Parameter `temperature`, `top_p` und `top_k`, über die sich der Grad der Kreativität der Antwort regeln lässt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b61c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(query):\n",
    "    response = requests.post(API_URL,\n",
    "                            headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "                            json={\"model\": MODEL,\n",
    "                                 \"messages\": [{\"role\": \"system\", \"content\": content},\n",
    "                                              {\"role\": \"user\", \"content\": query}],\n",
    "                                 \"max_tokens\": 500, # Legt die Anzahl der anzuzeigenden Token fest.\n",
    "                                 \"temperature\": 0.5, # Regelt Kreativität der Antwort. 1 kreativ, 0 konservativ\n",
    "                                 \"top_p\": 0.95, # Berücksichtigt alle Token, die zusammengenommen die Wahrscheinlichkeit p haben.\n",
    "                                 \"top_k\": 50}) # Berücksichtigt Anzahl k Token.\n",
    "    \n",
    "    response_json = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52eb94",
   "metadata": {},
   "source": [
    "Beim Aufruf der Funktion wird der Prompt als Parameter übergeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(\"Ich bin am Donnerstag halb 10 abends in Leipzig. Kann ich in der DNB arbeiten?\"))\n",
    "#print(llm(\"Ich arbeite zum Rauchwarenhandel in den 1920er Jahren. Welche Sets könnten für mich interessant sein?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244936a",
   "metadata": {},
   "source": [
    "Wer nicht allein die `print()`-Statements nutzen möchte, sondern lieber mehrere Fragen hintereinander stellen möchte, kann alternativ mit einer `while`-Schleife arbeiten. Mit `exit`, `quit` oder `q` als Eingabe wird das Programm beendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8efce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:    \n",
    "    user_input = input(\"Frage: \")\n",
    "    \n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Programm beendet.\")\n",
    "        break\n",
    "    \n",
    "    llm_output = llm(user_input)\n",
    "    print(\"Antwort:\", llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3375c0e",
   "metadata": {},
   "source": [
    "Über die `while`-Schleife kann man zwar nacheinander Fragen stellen, aber nicht klassisch \"chatten\". Zwei aufeinanderfolgende Fragen werden nicht in Beziehung miteinander gesetzt, da dem Modell der Kontext der bisherigen Konversation unbekannt ist. Um das zu ändern, müsste man den Inhalt von `user_input` sowie von `llm_output` jedes Mal in einer separaten Liste speichern und über die Variable `content` dem Modell mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b6612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
